{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a13HZOXUFJIF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from math import log2\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET                 = \"anime_girl\"\n",
        "START_TRAIN_AT_IMG_SIZE = 8 #The authors start from 8x8 images instead of 4x4\n",
        "DEVICE                  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE           = 1e-3\n",
        "BATCH_SIZES             = [256, 128, 64, 32, 16, 8]\n",
        "CHANNELS_IMG            = 3\n",
        "Z_DIM                   = 256\n",
        "W_DIM                   = 256\n",
        "IN_CHANNELS             = 256\n",
        "LAMBDA_GP               = 10\n",
        "PROGRESSIVE_EPOCHS      = [30] * len(BATCH_SIZES)\n"
      ],
      "metadata": {
        "id": "0cswOECyJa23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(image_size):\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.Normalize(\n",
        "                [0.5 for _ in range(CHANNELS_IMG)],\n",
        "                [0.5 for _ in range(CHANNELS_IMG)],\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    batch_size = BATCH_SIZES[int(log2(image_size / 4))]\n",
        "    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "    )\n",
        "    return loader, dataset"
      ],
      "metadata": {
        "id": "bxnaLEQdJkRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]"
      ],
      "metadata": {
        "id": "P3u4DvUiKe3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Noise Mapping Network\n",
        "class WSLinear(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_features, out_features,\n",
        "    ):\n",
        "        super(WSLinear, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "        self.scale = (2 / in_features)**0.5\n",
        "        self.bias = self.linear.bias\n",
        "        self.linear.bias = None\n",
        "\n",
        "        # initialize linear layer\n",
        "        nn.init.normal_(self.linear.weight)\n",
        "        nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x * self.scale) + self.bias\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PixelNorm, self).__init__()\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
        "\n",
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, z_dim, w_dim):\n",
        "        super().__init__()\n",
        "        self.mapping = nn.Sequential(\n",
        "            PixelNorm(),\n",
        "            WSLinear(z_dim, w_dim),\n",
        "            nn.ReLU(),\n",
        "            WSLinear(w_dim, w_dim),\n",
        "            nn.ReLU(),\n",
        "            WSLinear(w_dim, w_dim),\n",
        "            nn.ReLU(),\n",
        "            WSLinear(w_dim, w_dim),\n",
        "            nn.ReLU(),\n",
        "            WSLinear(w_dim, w_dim),\n",
        "            nn.ReLU(),\n",
        "            WSLinear(w_dim, w_dim),\n",
        "            nn.ReLU(),\n",
        "            WSLinear(w_dim, w_dim),\n",
        "            nn.ReLU(),\n",
        "            WSLinear(w_dim, w_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mapping(x)"
      ],
      "metadata": {
        "id": "8_YBQGqZKg9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adaptive Instance Normalization (AdaIN)\n",
        "class AdaIN(nn.Module):\n",
        "    def __init__(self, channels, w_dim):\n",
        "        super().__init__()\n",
        "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
        "        self.style_scale = WSLinear(w_dim, channels)\n",
        "        self.style_bias = WSLinear(w_dim, channels)\n",
        "\n",
        "    def forward(self, x, w):\n",
        "        x = self.instance_norm(x)\n",
        "        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)\n",
        "        style_bias = self.style_bias(w).unsqueeze(2).unsqueeze(3)\n",
        "        return style_scale * x + style_bias"
      ],
      "metadata": {
        "id": "AwUcVZEwcnG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inject Noise\n",
        "class InjectNoise(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        noise = torch.randn((x.shape[0], 1, x.shape[2], x.shape[3]), device=x.device)\n",
        "        return x + self.weight * noise"
      ],
      "metadata": {
        "id": "MS8g_tlqhLVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WSConv2d(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
        "    ):\n",
        "        super(WSConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.scale = (2 / (in_channels * (kernel_size ** 2))) ** 0.5\n",
        "        self.bias = self.conv.bias\n",
        "        self.conv.bias = None\n",
        "\n",
        "        # initialize conv layer\n",
        "        nn.init.normal_(self.conv.weight)\n",
        "        nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
      ],
      "metadata": {
        "id": "huFXhKSmhSD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
        "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
        "        self.leaky = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.leaky(self.conv1(x))\n",
        "        x = self.leaky(self.conv2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "_tr_LoJXhWiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels, img_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
        "        self.leaky = nn.LeakyReLU(0.2)\n",
        "\n",
        "        # here we work back ways from factors because the discriminator\n",
        "        # should be mirrored from the generator. So the first prog_block and\n",
        "        # rgb layer we append will work for input size 1024x1024, then 512->256-> etc\n",
        "        for i in range(len(factors) - 1, 0, -1):\n",
        "            conv_in = int(in_channels * factors[i])\n",
        "            conv_out = int(in_channels * factors[i - 1])\n",
        "            self.prog_blocks.append(ConvBlock(conv_in, conv_out))\n",
        "            self.rgb_layers.append(\n",
        "                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n",
        "            )\n",
        "\n",
        "        # perhaps confusing name \"initial_rgb\" this is just the RGB layer for 4x4 input size\n",
        "        # did this to \"mirror\" the generator initial_rgb\n",
        "        self.initial_rgb = WSConv2d(\n",
        "            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
        "        )\n",
        "        self.rgb_layers.append(self.initial_rgb)\n",
        "        self.avg_pool = nn.AvgPool2d(\n",
        "            kernel_size=2, stride=2\n",
        "        )  # down sampling using avg pool\n",
        "\n",
        "        # this is the block for 4x4 input size\n",
        "        self.final_block = nn.Sequential(\n",
        "            # +1 to in_channels because we concatenate from MiniBatch std\n",
        "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            WSConv2d(\n",
        "                in_channels, 1, kernel_size=1, padding=0, stride=1\n",
        "            ),  # we use this instead of linear layer\n",
        "        )\n",
        "\n",
        "    def fade_in(self, alpha, downscaled, out):\n",
        "        \"\"\"Used to fade in downscaled using avg pooling and output from CNN\"\"\"\n",
        "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
        "        return alpha * out + (1 - alpha) * downscaled\n",
        "\n",
        "    def minibatch_std(self, x):\n",
        "        batch_statistics = (\n",
        "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
        "        )\n",
        "        # we take the std for each example (across all channels, and pixels) then we repeat it\n",
        "        # for a single channel and concatenate it with the image. In this way the discriminator\n",
        "        # will get information about the variation in the batch/image\n",
        "        return torch.cat([x, batch_statistics], dim=1)\n",
        "\n",
        "    def forward(self, x, alpha, steps):\n",
        "        # where we should start in the list of prog_blocks, maybe a bit confusing but\n",
        "        # the last is for the 4x4. So example let's say steps=1, then we should start\n",
        "        # at the second to last because input_size will be 8x8. If steps==0 we just\n",
        "        # use the final block\n",
        "        cur_step = len(self.prog_blocks) - steps\n",
        "\n",
        "        # convert from rgb as initial step, this will depend on\n",
        "        # the image size (each will have it's on rgb layer)\n",
        "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
        "\n",
        "        if steps == 0:  # i.e, image is 4x4\n",
        "            out = self.minibatch_std(out)\n",
        "            return self.final_block(out).view(out.shape[0], -1)\n",
        "\n",
        "        # because prog_blocks might change the channels, for down scale we use rgb_layer\n",
        "        # from previous/smaller size which in our case correlates to +1 in the indexing\n",
        "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
        "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
        "\n",
        "        # the fade_in is done first between the downscaled and the input\n",
        "        # this is opposite from the generator\n",
        "        out = self.fade_in(alpha, downscaled, out)\n",
        "\n",
        "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
        "            out = self.prog_blocks[step](out)\n",
        "            out = self.avg_pool(out)\n",
        "\n",
        "        out = self.minibatch_std(out)\n",
        "        return self.final_block(out).view(out.shape[0], -1)"
      ],
      "metadata": {
        "id": "thg_x62UhYv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, w_dim, in_channels, img_channels=3):\n",
        "        super(Generator, self).__init__()\n",
        "        self.starting_constant = nn.Parameter(torch.ones((1, in_channels, 4, 4)))\n",
        "        self.map = MappingNetwork(z_dim, w_dim)\n",
        "        self.initial_adain1 = AdaIN(in_channels, w_dim)\n",
        "        self.initial_adain2 = AdaIN(in_channels, w_dim)\n",
        "        self.initial_noise1 = InjectNoise(in_channels)\n",
        "        self.initial_noise2 = InjectNoise(in_channels)\n",
        "        self.initial_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "        self.initial_rgb = WSConv2d(\n",
        "            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n",
        "        )\n",
        "        self.prog_blocks, self.rgb_layers = (\n",
        "            nn.ModuleList([]),\n",
        "            nn.ModuleList([self.initial_rgb]),\n",
        "        )\n",
        "\n",
        "        for i in range(len(factors) - 1):  # -1 to prevent index error because of factors[i+1]\n",
        "            conv_in_c = int(in_channels * factors[i])\n",
        "            conv_out_c = int(in_channels * factors[i + 1])\n",
        "            self.prog_blocks.append(GenBlock(conv_in_c, conv_out_c, w_dim))\n",
        "            self.rgb_layers.append(\n",
        "                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n",
        "            )\n",
        "\n",
        "    def fade_in(self, alpha, upscaled, generated):\n",
        "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
        "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
        "\n",
        "    def forward(self, noise, alpha, steps):\n",
        "        w = self.map(noise)\n",
        "        x = self.initial_adain1(self.initial_noise1(self.starting_constant), w)\n",
        "        x = self.initial_conv(x)\n",
        "        out = self.initial_adain2(self.leaky(self.initial_noise2(x)), w)\n",
        "\n",
        "        if steps == 0:\n",
        "            return self.initial_rgb(x)\n",
        "\n",
        "        for step in range(steps):\n",
        "            upscaled = F.interpolate(out, scale_factor=2, mode=\"bilinear\")\n",
        "            out = self.prog_blocks[step](upscaled, w)\n",
        "\n",
        "        # The number of channels in upscale will stay the same, while\n",
        "        # out which has moved through prog_blocks might change. To ensure\n",
        "        # we can convert both to rgb we use different rgb_layers\n",
        "        # (steps-1) and steps for upscaled, out respectively\n",
        "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
        "        final_out = self.rgb_layers[steps](out)\n",
        "        return self.fade_in(alpha, final_upscaled, final_out)"
      ],
      "metadata": {
        "id": "hjhDmvowh0Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_examples(gen, steps, n=100):\n",
        "\n",
        "    gen.eval()\n",
        "    alpha = 1.0\n",
        "    for i in range(n):\n",
        "        with torch.no_grad():\n",
        "            noise = torch.randn(1, Z_DIM).to(DEVICE)\n",
        "            img = gen(noise, alpha, steps)\n",
        "            if not os.path.exists(f'saved_examples/step{steps}'):\n",
        "                os.makedirs(f'saved_examples/step{steps}')\n",
        "            save_image(img*0.5+0.5, f\"saved_examples/step{steps}/img_{i}.png\")\n",
        "    gen.train()"
      ],
      "metadata": {
        "id": "GWY_UPs8h-Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
        "    interpolated_images.requires_grad_(True)\n",
        "\n",
        "    # Calculate critic scores\n",
        "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
        "\n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty"
      ],
      "metadata": {
        "id": "ZAI13f6hh_ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = Generator(\n",
        "        Z_DIM, W_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG\n",
        "    ).to(DEVICE)\n",
        "critic = Discriminator(IN_CHANNELS, img_channels=CHANNELS_IMG).to(DEVICE)\n",
        "# initialize optimizers\n",
        "opt_gen = optim.Adam([{\"params\": [param for name, param in gen.named_parameters() if \"map\" not in name]},\n",
        "                        {\"params\": gen.map.parameters(), \"lr\": 1e-5}], lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
        "opt_critic = optim.Adam(\n",
        "    critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99)\n",
        ")\n",
        "\n",
        "\n",
        "gen.train()\n",
        "critic.train()\n",
        "\n",
        "# start at step that corresponds to img size that we set in config\n",
        "step = int(log2(START_TRAIN_AT_IMG_SIZE / 4))\n",
        "for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
        "    alpha = 1e-5   # start with very low alpha\n",
        "    loader, dataset = get_loader(4 * 2 ** step)\n",
        "    print(f\"Current image size: {4 * 2 ** step}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        alpha = train_fn(\n",
        "            critic,\n",
        "            gen,\n",
        "            loader,\n",
        "            dataset,\n",
        "            step,\n",
        "            alpha,\n",
        "            opt_critic,\n",
        "            opt_gen\n",
        "        )\n",
        "\n",
        "    generate_examples(gen, step)\n",
        "    step += 1  # progress to the next img size"
      ],
      "metadata": {
        "id": "ZbeESkERi9sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(\n",
        "    critic,\n",
        "    gen,\n",
        "    loader,\n",
        "    dataset,\n",
        "    step,\n",
        "    alpha,\n",
        "    opt_critic,\n",
        "    opt_gen,\n",
        "):\n",
        "    loop = tqdm(loader, leave=True)\n",
        "\n",
        "    for batch_idx, (real, _) in enumerate(loop):\n",
        "        real = real.to(DEVICE)\n",
        "        cur_batch_size = real.shape[0]\n",
        "\n",
        "\n",
        "        noise = torch.randn(cur_batch_size, Z_DIM).to(DEVICE)\n",
        "\n",
        "        fake = gen(noise, alpha, step)\n",
        "        critic_real = critic(real, alpha, step)\n",
        "        critic_fake = critic(fake.detach(), alpha, step)\n",
        "        gp = gradient_penalty(critic, real, fake, alpha, step, device=DEVICE)\n",
        "        loss_critic = (\n",
        "            -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
        "            + LAMBDA_GP * gp\n",
        "            + (0.001 * torch.mean(critic_real ** 2))\n",
        "        )\n",
        "\n",
        "        critic.zero_grad()\n",
        "        loss_critic.backward()\n",
        "        opt_critic.step()\n",
        "\n",
        "        gen_fake = critic(fake, alpha, step)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Update alpha and ensure less than 1\n",
        "        alpha += cur_batch_size / (\n",
        "            (PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n",
        "        )\n",
        "        alpha = min(alpha, 1)\n",
        "\n",
        "        loop.set_postfix(\n",
        "            gp=gp.item(),\n",
        "            loss_critic=loss_critic.item(),\n",
        "        )\n",
        "\n",
        "\n",
        "    return alpha"
      ],
      "metadata": {
        "id": "oDcX3jmAi1KM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}